{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trainデータセットとvalidデータセットを用いて、テキスト分類モデルの実装しなさい(学習が遅い場合、一部のtrainデータセットだけ使っても構いません)\n",
    "- testデータセットでモデルの精度を検証しなさい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把三个文件的数据取出来整理成一个数据集\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "valid_df = pd.read_csv(\"./valid.csv\")\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "data_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'valid': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.set_format(type=\"pandas\")\n",
    "train_df = data_dict[\"train\"][:]\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(4, 6))\n",
    "data_dict[\"train\"][\"label\"].value_counts(ascending=True).plot(kind=\"barh\", title=\"Train Dataset\", ax = axes[0])\n",
    "data_dict[\"valid\"][\"label\"].value_counts(ascending=True).plot(kind=\"barh\", title=\"Valid Dataset\", ax = axes[1])\n",
    "data_dict[\"test\"][\"label\"].value_counts(ascending=True).plot(kind=\"barh\", title=\"Test Dataset\", ax = axes[2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text_length\"]=train_df[\"sentence\"].str.len()\n",
    "train_df.boxplot(column=\"text_length\", by=\"label\", figsize=(3, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用模型的分词器进行分词\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')\n",
    "\n",
    "sample_text_encoded = tokenizer(train_df[\"sentence\"][0])\n",
    "sample_tokens = tokenizer.convert_ids_to_tokens(sample_text_encoded.input_ids)\n",
    "\n",
    "print(sample_text_encoded)\n",
    "print(sample_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在整个数据集上应用分词器\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "data_dict.reset_format()\n",
    "\n",
    "data_dict = data_dict.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoded = data_dict[\"train\"][0]\n",
    "pd.DataFrame(\n",
    "    [sample_encoded[\"input_ids\"]\n",
    "     , sample_encoded[\"attention_mask\"]\n",
    "     , tokenizer.convert_ids_to_tokens(sample_encoded[\"input_ids\"])],\n",
    "    ['input_ids', 'attention_mask', \"tokens\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查cuda是否可用\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! :D\")\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available. :(\")\n",
    "\n",
    "# 如果安装了cuda还是不可用，可能是cuda和pytorch的版本没有对应上，比如安装了pytorch的cpu版本\n",
    "# 如果真是版本问题，建议删除当前环境的cuda，pytorch等相关的库，使用conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia 或者 pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118重新安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练所需参数\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(data_dict[\"train\"]) // batch_size\n",
    "\n",
    "# 设置模型输出的存储位置\n",
    "model_name = r\"G:\\result\"\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    "    log_level=\"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行设备的设置，如果cuda可用，则使用gpu训练，不可用则使用cpu\n",
    "# 调用预训练的bert模型\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2\n",
    "\n",
    "model = (AutoModelForSequenceClassification\n",
    "    .from_pretrained(\"cl-tohoku/bert-base-japanese-v3\", num_labels=num_labels)\n",
    "    .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型的性能指标\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练所需参数\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(data_dict[\"train\"]) // batch_size\n",
    "\n",
    "# 设置模型输出的存储位置\n",
    "model_name = r\"G:\\result\"\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    "    log_level=\"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试输出目录是否可用，不可用则生成目录\n",
    "import os\n",
    "\n",
    "\n",
    "test_dir = r\"G:\\result\"\n",
    "\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "test_file_path = os.path.join(test_dir, 'test_file.txt')\n",
    "try:\n",
    "    with open(test_file_path, 'w') as file:\n",
    "        file.write('Hello, this is a test file.')\n",
    "    print(\"文件写入成功，目录可写。\")\n",
    "except Exception as e:\n",
    "    print(f\"写入文件时发生错误：{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data_dict[\"train\"],\n",
    "    eval_dataset=data_dict[\"valid\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(data_dict[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行预测\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_valid = np.array(data_dict[\"test\"][\"label\"])\n",
    "labels = ['0', '1']\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
